{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = \"data/base/goodreads_train.csv\"\n",
    "result_path = \"data/base/goodreads_test.csv\"\n",
    "\n",
    "df = pd.read_csv(train_path)\n",
    "\n",
    "df_train = df.drop(columns=['user_id', 'book_id', 'date_added', 'date_updated',\n",
    "                            'read_at', 'started_at', 'n_votes', 'n_comments'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\enzol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_review(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    text = re.sub(r'(view spoiler|hide spoiler)', '', text)\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    clean_tokens = [WordNetLemmatizer().lemmatize(tok) for tok in tokens if\n",
    "                    tok not in stopwords_english and len(tok) > 1]\n",
    "    # clean_tokens = [tok for tok in tokens if tok not in stopwords_english and len(tok) > 1]\n",
    "\n",
    "    clean_text = ' '.join(clean_tokens)\n",
    "\n",
    "    return clean_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 50s\n",
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_train[\"clean_text\"] = df_train[\"review_text\"].apply(preprocess_review)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (720000,) (720000,)\n",
      "Validation data: (180000,) (180000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train = df_train[\"clean_text\"].values\n",
    "y_train = df_train[\"rating\"].values\n",
    "\n",
    "x_tr, x_va, y_tr, y_va = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Training data:\", x_tr.shape, y_tr.shape)\n",
    "print(\"Validation data:\", x_va.shape, y_va.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(x_tr)\n",
    "\n",
    "max_seq_length = 250\n",
    "\n",
    "x_tr_seq = tokenizer.texts_to_sequences(x_tr)\n",
    "x_tr_seq = pad_sequences(x_tr_seq, maxlen=max_seq_length)\n",
    "\n",
    "x_va_seq = tokenizer.texts_to_sequences(x_va)\n",
    "x_va_seq = pad_sequences(x_va_seq, maxlen=max_seq_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 250, 128)          2560000   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 248, 64)           24640     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 124, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 124, 128)         66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 124, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 124, 128)          0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 256)              263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,923,942\n",
      "Trainable params: 2,923,110\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=20000, output_dim=128, input_length=250))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=128)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=6, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics='sparse_categorical_accuracy'\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "earlystopping_cb = EarlyStopping(patience=6, restore_best_weights=True)\n",
    "tensorboard = TensorBoard(\"logs/tests/kaggle\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1407/1407 [==============================] - 84s 57ms/step - loss: 1.5128 - sparse_categorical_accuracy: 0.4136 - val_loss: 1.1723 - val_sparse_categorical_accuracy: 0.5135\n",
      "Epoch 2/50\n",
      "1407/1407 [==============================] - 79s 56ms/step - loss: 1.1724 - sparse_categorical_accuracy: 0.5158 - val_loss: 1.1498 - val_sparse_categorical_accuracy: 0.5119\n",
      "Epoch 3/50\n",
      "1407/1407 [==============================] - 78s 55ms/step - loss: 1.0845 - sparse_categorical_accuracy: 0.5414 - val_loss: 1.0803 - val_sparse_categorical_accuracy: 0.5405\n",
      "Epoch 4/50\n",
      "1407/1407 [==============================] - 75s 53ms/step - loss: 1.0276 - sparse_categorical_accuracy: 0.5652 - val_loss: 1.1388 - val_sparse_categorical_accuracy: 0.5252\n",
      "Epoch 5/50\n",
      "1407/1407 [==============================] - 77s 55ms/step - loss: 0.9772 - sparse_categorical_accuracy: 0.5876 - val_loss: 1.0796 - val_sparse_categorical_accuracy: 0.5434\n",
      "Epoch 6/50\n",
      "1407/1407 [==============================] - 78s 55ms/step - loss: 0.9253 - sparse_categorical_accuracy: 0.6136 - val_loss: 1.1187 - val_sparse_categorical_accuracy: 0.5389\n",
      "Epoch 7/50\n",
      "1407/1407 [==============================] - 78s 55ms/step - loss: 0.8719 - sparse_categorical_accuracy: 0.6405 - val_loss: 1.2279 - val_sparse_categorical_accuracy: 0.5222\n",
      "Epoch 8/50\n",
      "1407/1407 [==============================] - 78s 56ms/step - loss: 0.8179 - sparse_categorical_accuracy: 0.6685 - val_loss: 1.2931 - val_sparse_categorical_accuracy: 0.5042\n",
      "Epoch 9/50\n",
      "1407/1407 [==============================] - 80s 57ms/step - loss: 0.7646 - sparse_categorical_accuracy: 0.6954 - val_loss: 1.2957 - val_sparse_categorical_accuracy: 0.5218\n",
      "Epoch 10/50\n",
      "1407/1407 [==============================] - 79s 56ms/step - loss: 0.7146 - sparse_categorical_accuracy: 0.7203 - val_loss: 1.3373 - val_sparse_categorical_accuracy: 0.5185\n",
      "Epoch 11/50\n",
      "1407/1407 [==============================] - 78s 55ms/step - loss: 0.6697 - sparse_categorical_accuracy: 0.7415 - val_loss: 1.4012 - val_sparse_categorical_accuracy: 0.5114\n",
      "CPU times: total: 17min 46s\n",
      "Wall time: 14min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(x_tr_seq,\n",
    "                    y_tr,\n",
    "                    validation_data=(x_va_seq, y_va),\n",
    "                    callbacks=[earlystopping_cb, tensorboard],\n",
    "                    batch_size=512,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 user_id   book_id  \\\n413805  cbe851ae20560fc12501dc98af876fe3  12384972   \n\n                               review_id  \\\n413805  80f6dbe3b5cc8d048e7ed492cd768328   \n\n                                              review_text  \\\n413805  oh man, i am so glad i read this. i was workin...   \n\n                            date_added                    date_updated  \\\n413805  Mon Jun 17 16:04:36 -0700 2013  Sat Jun 29 18:35:26 -0700 2013   \n\n                               read_at                      started_at  \\\n413805  Fri Jun 21 00:00:00 -0700 2013  Mon Jun 17 00:00:00 -0700 2013   \n\n        n_votes  n_comments  \n413805        1           1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>413805</th>\n      <td>cbe851ae20560fc12501dc98af876fe3</td>\n      <td>12384972</td>\n      <td>80f6dbe3b5cc8d048e7ed492cd768328</td>\n      <td>oh man, i am so glad i read this. i was workin...</td>\n      <td>Mon Jun 17 16:04:36 -0700 2013</td>\n      <td>Sat Jun 29 18:35:26 -0700 2013</td>\n      <td>Fri Jun 21 00:00:00 -0700 2013</td>\n      <td>Mon Jun 17 00:00:00 -0700 2013</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(result_path, sep=\",\")\n",
    "\n",
    "df_test.sample()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 16s\n",
      "Wall time: 4min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": "                          review_id  \\\n0  5c4df7e70e9b438c761f07a4620ccb7c   \n1  8eaeaf13213eeb16ad879a2a2591bbe5   \n2  dce649b733c153ba5363a0413cac988f   \n3  8a46df0bb997269d6834f9437a4b0a77   \n4  d11d3091e22f1cf3cb865598de197599   \n\n                                          clean_text  \n0  spoiler alert definitely one favorite among fo...  \n1  spoiler alert drink im huge fan coffee dont bu...  \n2  roar one favorite character never sky im happy...  \n3  spoiler alert feel like travelling europe dont...  \n4  star read enjoyed first two novel series say b...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5c4df7e70e9b438c761f07a4620ccb7c</td>\n      <td>spoiler alert definitely one favorite among fo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8eaeaf13213eeb16ad879a2a2591bbe5</td>\n      <td>spoiler alert drink im huge fan coffee dont bu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dce649b733c153ba5363a0413cac988f</td>\n      <td>roar one favorite character never sky im happy...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8a46df0bb997269d6834f9437a4b0a77</td>\n      <td>spoiler alert feel like travelling europe dont...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d11d3091e22f1cf3cb865598de197599</td>\n      <td>star read enjoyed first two novel series say b...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_test = df_test.drop(columns=['user_id', 'book_id', 'date_added', 'date_updated',\n",
    "                                'read_at', 'started_at', 'n_votes', 'n_comments'])\n",
    "\n",
    "df_test[\"clean_text\"] = df_test[\"review_text\"].apply(preprocess_review)\n",
    "\n",
    "df_test = df_test.drop(columns=['review_text'])\n",
    "\n",
    "df_test.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "x_te = df_test[\"clean_text\"].values\n",
    "\n",
    "x_te_seq = tokenizer.texts_to_sequences(x_te)\n",
    "x_te_seq = pad_sequences(x_te_seq, maxlen=max_seq_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14939/14939 [==============================] - 150s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               review_id  rating\n14318   825139461ff6c8e419fa7390a108a902       3\n340129  596f74a9fb82e284cd9415eacf16a53e       5\n61225   b9a74113fda274f7756a4e2c58d30594       5\n90668   b203d43a92adb62e840fe775375c4ce5       4\n90210   a4c943be8f9ca7d570ffded07f69893f       4\n58839   a3ed506ebe9aef44a6823526da44ec8e       5\n240846  ffa00db0748f3d3b66e262755bb7dce0       4\n462700  9839e90bf1914a385479f9854bdfb4fb       4\n325108  3cb084527e36990413af5d1672ea678a       4\n136329  6d8e5a790569b562a0ba9476b449a9cf       4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14318</th>\n      <td>825139461ff6c8e419fa7390a108a902</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>340129</th>\n      <td>596f74a9fb82e284cd9415eacf16a53e</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>61225</th>\n      <td>b9a74113fda274f7756a4e2c58d30594</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>90668</th>\n      <td>b203d43a92adb62e840fe775375c4ce5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>90210</th>\n      <td>a4c943be8f9ca7d570ffded07f69893f</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>58839</th>\n      <td>a3ed506ebe9aef44a6823526da44ec8e</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>240846</th>\n      <td>ffa00db0748f3d3b66e262755bb7dce0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>462700</th>\n      <td>9839e90bf1914a385479f9854bdfb4fb</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>325108</th>\n      <td>3cb084527e36990413af5d1672ea678a</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>136329</th>\n      <td>6d8e5a790569b562a0ba9476b449a9cf</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = [np.argmax(i) for i in model.predict(x_te_seq)]\n",
    "\n",
    "# Create a new DataFrame to merge review ids and the model predictions\n",
    "submission = pd.DataFrame({'review_id': df_test.review_id, 'rating': predictions})\n",
    "\n",
    "# Check few random entries\n",
    "submission.sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "submission.to_csv(\"res_files/submission.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625/5625 [==============================] - 63s 11ms/step - loss: 1.0796 - sparse_categorical_accuracy: 0.5434\n",
      "Validation loss: 1.079634428024292\n",
      "Validation accuracy: 0.5434277653694153\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_va_seq,y_va)\n",
    "\n",
    "print('Validation loss:', test_loss)\n",
    "print('Validation accuracy:', test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
