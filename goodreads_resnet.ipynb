{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from utils import predicted_test_data_to_result_csv\n",
    "from keras import layers, losses, Input, Model\n",
    "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Activation, Flatten\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.metrics import sparse_categorical_accuracy\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "train_path = \"data/base/goodreads_train.csv\"\n",
    "result_path = \"data/base/goodreads_test.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_path, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop all lines with 0 in rating column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "index = df[(df['rating'] == 0)].index\n",
    "df.drop(index, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Targets DataFrames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "targets = df.pop('rating')\n",
    "targets = targets - 1\n",
    "# targets = tf.keras.utils.to_categorical(targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Features DataFrames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(869012, 1), dtype=string, numpy=\narray([[b'This is a special book. It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind. This is what I love about good science fiction - it pushes your thinking about where things can go. \\n It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I\\'ve read. For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc. \\n It is a book about science, and aliens. The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell. (view spoiler)[Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though. \\n But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned? That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together. \\n I did like how the book unveiled the Trisolaran culture through the game. It was a smart way to build empathy with them and also understand what they\\'ve gone through across so many centuries. And who know a 3 body problem was an unsolvable math problem? But I still don\\'t get who made the game - maybe that will come in the next book. \\n I loved this quote: \\n \"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists? How many neutrons and electrons? Probably no fewer than a hundred million. Every collision was probably the end of the civilizations and intelligences in a microcosmos. In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons. Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\" \\n (hide spoiler)]'],\n       [b'Recommended by Don Katz. Avail for free in December: http://www.audible.com/mt/ellison2?so...'],\n       [b'A fun, fast paced science fiction thriller. I read it in 2 nights and couldn\\'t put it down. The book is about the quantum theory of many worlds which states that all decisions we make throughout our lives basically create branches, and that each possible path through the decision tree can be thought of as a parallel world. And in this book, someone invents a way to switch between these worlds. This was nicely alluded to/foreshadowed in this quote: \\n \"I think about all the choices we\\'ve made that created this moment. Us sitting here together at this beautiful table. Then I think of all the possible events that could have stopped this moment from ever happening, and it all feels, I don\\'t know...\" \"What?\" \"So fragile.\" Now he becomes thoughtful for a moment. He says finally, \"It\\'s terrifying when you consider that every thought we have, every choice we could possibly make, branches into a new world.\" \\n (view spoiler)[This book can\\'t be discussed without spoilers. It is a book about choice and regret. Ever regret not chasing the girl of your dreams so you can focus on your career? Well Jason2 made that choice and then did regret it. Clearly the author is trying to tell us to optimize for happiness - to be that second rate physics teacher at a community college if it means you can have a happy life. I\\'m being snarky because while there is certainly something to that, you also have to have meaning in your life that comes from within. I thought the book was a little shallow on this dimension. In fact, all the characters were fairly shallow. Daniela was the perfect wife. Ryan the perfect antithesis of Jason. Amanda the perfect loyal traveling companion, etc. This, plus the fact that the book was weak on the science are what led me to take a few stars off - but I\\'d still read it again if I could go back in time - was a very fun and engaging read. \\n If you want to really minimize regret, you have to live your life to avoid it in the first place. Regret can\\'t be hacked, which is kind of the point of the book. My favorite book about regret is Remains of the Day. I do really like the visualization of the decision tree though - that is a powerful concept. \\n \"Every moment, every breath, contains a choice. But life is imperfect. We make the wrong choices. So we end up living in a state of perpetual regret, and is there anything worse? I built something that could actually eradicate regret. Let you find worlds where you made the right choice.\" Daniela says, \"Life doesn\\'t work that way. You live with your choices and learn. You don\\'t cheat the system.\" \\n (hide spoiler)]'],\n       ...,\n       [b'** spoiler alert ** \\n 3.5 stars. \\n This book is sweet inside and out! What\\'s sweeter than the ice cream on this cover??? and Patrick...he\\'s such a sweet guy! \\n Elyse hates Valentine\\'s Day, because a year ago, on that very same day she was betrayed by the two people she loved. She caught her best friend and boyfriend cheating on her! Now, she is working for a gift/card store with her new friend Dina and seriously annoyed by the singing Cupid that every customer seemed to love. After her last relationship, she vowed not to be involved with anyone for a while and decided to focus in school and work. Dina, on the other hand, can\\'t seem to get over her ex-boyfriend. Elyse wants to help her friend to forget her ex, so when a cute guy came in the store, she believe that she just found the perfect distraction for Dina. The cute guy---Patrick have a mind and feelings of his own, and he\\'s on a mission to show Elyse that he\\'s nothing like her ex-boyfriend! \\n Elyse was a little annoying, however, I understand where she is coming from. I guess if you were betrayed and got hurt, your initial reaction is to protect yourself. You\\'ll be scared to take risks and you tend to push people away. It\\'s also harder to trust and open up to people. That\\'s what Elyse was doing in this story. When she and her mom moved because of financial reasons (this is right after \"the break-up\"), she never made friends in her new school, aside from Dina. That\\'s why when she noticed Patrick\\'s advances, she immediately shut him out. I just love Patrick. He\\'s too good to be true! He\\'s sweet and thoughtful, and I totally hate Elyse now for having him. LOL! \\n I liked the story and the characters,the book made me smile despite my sickness. This book is perfect for people who wants to read light-hearted teen romance novels. \\n Now I\\'m off to find MY Patrick! =D'],\n       [b'** spoiler alert ** \\n Another fun read from Ms Evanovich! \\n Diesel and Lizzy\\'s new assignment is to find the next SALIGIA stone which is Luxuria also known as the \"lust stone\". Of course Wulf and his crazy assistant Hatchett is also looking for the same stone, but there\\'s a new player in the mix---Anarchy. \\n There was a lot of action that occured in this book than in Wicked Appetite. As always, Glo and the Carl\\'s antics made me laugh. I\\'m glad that this two characters were used in finding the clues that lead Diesel and Lizzy closer to the Luxuria stone. I was surprised that Carl was the \"innocent\" one when he loves flipping people off. LOL! I really enjoyed their scavenger hunt. The Hatchett-Glo love team was hilarious. I just wish that it was Diesel or Wulf who got affected by the Luxuria and not Hatchet. Morty and his spoon bending ability was funny too! I think my favorite addition in this book is the exploding cars! LOL. I still want to know more about Wulf, It seems like he\\'s really not that bad. I hope we get to know him better in the next book! \\n I had fun reading this book. Even though I was feeling under the weather, this book kept me well entertained and awake all through out my work shift. Recommended for people who are looking for something light and funny book to read.'],\n       [b\"** spoiler alert ** \\n 3.5 stars \\n I liked it! The story is original and it's well written. I find Jane a very likable character, she's smart and confident. However, I just don't get what she liked so much about Elton, he's so not worth it! The other characters are not so memorable. Her best friend for example, her character is flat. I didn't learn much about her to be sad or frustrated when she supposedly lost her soul to Lanalee. I liked Owen though, even if he's more than a hundred years old, he's still charming and swoon worthy. Though I liked the idea of him and Jane fighting demons together, I just wished they had more time getting to know each other before they decided to be boyfriend-girlfriend. \\n Now my question is: why there isn't a sequel for Devilish? \\n This was a quick and entertaining read. I recommend it for YA paranormal fans who are looking for something light and unpredictable read.\"]],\n      dtype=object)>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = ['review_text']\n",
    "features = df[features_names]\n",
    "tf.convert_to_tensor(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_spoilers = tf.strings.regex_replace(lowercase, '\\*\\* spoiler alert \\*\\*', ' ')\n",
    "    return tf.strings.regex_replace(stripped_spoilers,\n",
    "                                    '[%s]' % re.escape(string.punctuation),\n",
    "                                    '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "max_features = 5000  # Maximum vocab size.\n",
    "sequence_length = 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "vectorized_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "vectorized_layer.adapt(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "model_nb = 2\n",
    "\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.0001\n",
    "batch_size = 8000\n",
    "dropout_rate = 0.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_7 (TextVect  (None, 100)         0           ['input_8[0][0]']                \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 100, 50)      250050      ['text_vectorization_7[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 98, 64)       9664        ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 98, 64)      256         ['conv1d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 98, 64)       0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 94, 128)      41088       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 94, 128)     512         ['conv1d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 94, 128)      0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 90, 256)      164096      ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 90, 256)      141056      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 90, 256)     1024        ['conv1d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 90, 256)     1024        ['conv1d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 90, 256)     0           ['batch_normalization_18[0][0]', \n",
      " mbda)                                                            'batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 256)         0           ['tf.__operators__.add_4[0][0]'] \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           8224        ['global_max_pooling1d_4[0][0]'] \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 5)            165         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 617,159\n",
      "Trainable params: 615,751\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "vectorized_text = vectorized_layer(input_text)\n",
    "\n",
    "embedding_layer = Embedding(max_features + 1, embedding_dim, input_length=sequence_length)(vectorized_text)\n",
    "\n",
    "x_shortcut = embedding_layer\n",
    "\n",
    "#### Main path ####\n",
    "# First\n",
    "x = Conv1D(64, 3, activation='relu', padding = 'valid')(embedding_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Second\n",
    "x = Conv1D(128, 5, activation='relu', padding = 'valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Third\n",
    "x = Conv1D(256, 5, activation='relu', padding = 'valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#### Shortcut path ####\n",
    "x_shortcut = Conv1D(256, 11, activation='relu', padding = 'valid')(x_shortcut)\n",
    "x_shortcut = BatchNormalization()(x_shortcut)\n",
    "\n",
    "# x and x_shortcut addition\n",
    "x = x + x_shortcut\n",
    "\n",
    "global_max_pooling = GlobalMaxPooling1D()(x)\n",
    "\n",
    "relu = Dense(32, activation='relu')(global_max_pooling)\n",
    "\n",
    "output = Dense(5, activation='softmax')(relu)\n",
    "\n",
    "resnet_model = Model(input_text, output)\n",
    "\n",
    "resnet_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "resnet_model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate=learning_rate),\n",
    "                  metrics=sparse_categorical_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "exp_name = f'resnet_model_{model_nb}_lr_{learning_rate}_bs_{batch_size}_dr_{dropout_rate}'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "82/82 [==============================] - 70s 804ms/step - loss: 2.1018 - sparse_categorical_accuracy: 0.2950 - val_loss: 1.4902 - val_sparse_categorical_accuracy: 0.3140\n",
      "Epoch 2/50\n",
      "82/82 [==============================] - 64s 780ms/step - loss: 1.4809 - sparse_categorical_accuracy: 0.3578 - val_loss: 1.4810 - val_sparse_categorical_accuracy: 0.3140\n",
      "Epoch 3/50\n",
      "82/82 [==============================] - 64s 780ms/step - loss: 1.3872 - sparse_categorical_accuracy: 0.3928 - val_loss: 1.4809 - val_sparse_categorical_accuracy: 0.3140\n",
      "Epoch 4/50\n",
      "82/82 [==============================] - 64s 779ms/step - loss: 1.3229 - sparse_categorical_accuracy: 0.4215 - val_loss: 1.4710 - val_sparse_categorical_accuracy: 0.3140\n",
      "Epoch 5/50\n",
      "82/82 [==============================] - 64s 777ms/step - loss: 1.2727 - sparse_categorical_accuracy: 0.4441 - val_loss: 1.4405 - val_sparse_categorical_accuracy: 0.3141\n",
      "Epoch 6/50\n",
      "82/82 [==============================] - 64s 777ms/step - loss: 1.2303 - sparse_categorical_accuracy: 0.4637 - val_loss: 1.3718 - val_sparse_categorical_accuracy: 0.3309\n",
      "Epoch 7/50\n",
      "82/82 [==============================] - 64s 777ms/step - loss: 1.1931 - sparse_categorical_accuracy: 0.4809 - val_loss: 1.2884 - val_sparse_categorical_accuracy: 0.3907\n",
      "Epoch 8/50\n",
      "82/82 [==============================] - 64s 777ms/step - loss: 1.1601 - sparse_categorical_accuracy: 0.4963 - val_loss: 1.2276 - val_sparse_categorical_accuracy: 0.4434\n",
      "Epoch 9/50\n",
      "82/82 [==============================] - 64s 780ms/step - loss: 1.1302 - sparse_categorical_accuracy: 0.5102 - val_loss: 1.1978 - val_sparse_categorical_accuracy: 0.4612\n",
      "Epoch 10/50\n",
      "82/82 [==============================] - 65s 791ms/step - loss: 1.1025 - sparse_categorical_accuracy: 0.5231 - val_loss: 1.1842 - val_sparse_categorical_accuracy: 0.4691\n",
      "Epoch 11/50\n",
      "82/82 [==============================] - 65s 790ms/step - loss: 1.0785 - sparse_categorical_accuracy: 0.5347 - val_loss: 1.1760 - val_sparse_categorical_accuracy: 0.4731\n",
      "Epoch 12/50\n",
      "82/82 [==============================] - 64s 787ms/step - loss: 1.0577 - sparse_categorical_accuracy: 0.5445 - val_loss: 1.1682 - val_sparse_categorical_accuracy: 0.4771\n",
      "Epoch 13/50\n",
      "82/82 [==============================] - 64s 787ms/step - loss: 1.0392 - sparse_categorical_accuracy: 0.5536 - val_loss: 1.1614 - val_sparse_categorical_accuracy: 0.4799\n",
      "Epoch 14/50\n",
      "82/82 [==============================] - 65s 789ms/step - loss: 1.0221 - sparse_categorical_accuracy: 0.5626 - val_loss: 1.1552 - val_sparse_categorical_accuracy: 0.4826\n",
      "Epoch 15/50\n",
      "82/82 [==============================] - 66s 802ms/step - loss: 1.0063 - sparse_categorical_accuracy: 0.5707 - val_loss: 1.1511 - val_sparse_categorical_accuracy: 0.4841\n",
      "Epoch 16/50\n",
      "82/82 [==============================] - 65s 799ms/step - loss: 0.9914 - sparse_categorical_accuracy: 0.5785 - val_loss: 1.1468 - val_sparse_categorical_accuracy: 0.4862\n",
      "Epoch 17/50\n",
      "82/82 [==============================] - 65s 792ms/step - loss: 0.9772 - sparse_categorical_accuracy: 0.5865 - val_loss: 1.1442 - val_sparse_categorical_accuracy: 0.4883\n",
      "Epoch 18/50\n",
      "82/82 [==============================] - 67s 819ms/step - loss: 0.9639 - sparse_categorical_accuracy: 0.5934 - val_loss: 1.1428 - val_sparse_categorical_accuracy: 0.4888\n",
      "Epoch 19/50\n",
      "82/82 [==============================] - 68s 833ms/step - loss: 0.9508 - sparse_categorical_accuracy: 0.6004 - val_loss: 1.1421 - val_sparse_categorical_accuracy: 0.4891\n",
      "Epoch 20/50\n",
      "82/82 [==============================] - 67s 819ms/step - loss: 0.9381 - sparse_categorical_accuracy: 0.6073 - val_loss: 1.1432 - val_sparse_categorical_accuracy: 0.4891\n",
      "Epoch 21/50\n",
      "82/82 [==============================] - 68s 832ms/step - loss: 0.9257 - sparse_categorical_accuracy: 0.6138 - val_loss: 1.1446 - val_sparse_categorical_accuracy: 0.4903\n",
      "Epoch 22/50\n",
      "82/82 [==============================] - 68s 833ms/step - loss: 0.9137 - sparse_categorical_accuracy: 0.6202 - val_loss: 1.1455 - val_sparse_categorical_accuracy: 0.4914\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - 68s 836ms/step - loss: 0.9018 - sparse_categorical_accuracy: 0.6267 - val_loss: 1.1479 - val_sparse_categorical_accuracy: 0.4908\n",
      "Epoch 24/50\n",
      "76/82 [==========================>...] - ETA: 3s - loss: 0.8886 - sparse_categorical_accuracy: 0.6336"
     ]
    }
   ],
   "source": [
    "resnet_model.fit(features,\n",
    "              targets,\n",
    "              validation_split=0.25,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[tf.keras.callbacks.TensorBoard(\"logs/resnets/\" + exp_name)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(result_path, sep=\",\")\n",
    "\n",
    "df_test_modified = df_test.drop(columns=[\n",
    "    'user_id',\n",
    "    'book_id',\n",
    "    'review_id',\n",
    "    'date_added',\n",
    "    'date_updated',\n",
    "    'read_at',\n",
    "    'started_at',\n",
    "    'n_votes',\n",
    "    'n_comments'\n",
    "], inplace=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_test_data = resnet_model.predict(df_test_modified)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_test_data_to_result_csv(df_test, predicted_test_data, exp_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
