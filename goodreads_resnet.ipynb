{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils import predicted_test_data_to_result_csv\n",
    "from keras import layers, losses, Input, Model\n",
    "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Activation, Flatten\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.metrics import sparse_categorical_accuracy\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "train_path = \"data/base/goodreads_train.csv\"\n",
    "result_path = \"data/base/goodreads_test.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_path, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "targets = df.pop('rating')\n",
    "# targets = tf.keras.utils.to_categorical(targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(869012, 1), dtype=string, numpy=\narray([[b'This is a special book. It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind. This is what I love about good science fiction - it pushes your thinking about where things can go. \\n It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I\\'ve read. For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc. \\n It is a book about science, and aliens. The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell. (view spoiler)[Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though. \\n But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned? That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together. \\n I did like how the book unveiled the Trisolaran culture through the game. It was a smart way to build empathy with them and also understand what they\\'ve gone through across so many centuries. And who know a 3 body problem was an unsolvable math problem? But I still don\\'t get who made the game - maybe that will come in the next book. \\n I loved this quote: \\n \"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists? How many neutrons and electrons? Probably no fewer than a hundred million. Every collision was probably the end of the civilizations and intelligences in a microcosmos. In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons. Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\" \\n (hide spoiler)]'],\n       [b'Recommended by Don Katz. Avail for free in December: http://www.audible.com/mt/ellison2?so...'],\n       [b'A fun, fast paced science fiction thriller. I read it in 2 nights and couldn\\'t put it down. The book is about the quantum theory of many worlds which states that all decisions we make throughout our lives basically create branches, and that each possible path through the decision tree can be thought of as a parallel world. And in this book, someone invents a way to switch between these worlds. This was nicely alluded to/foreshadowed in this quote: \\n \"I think about all the choices we\\'ve made that created this moment. Us sitting here together at this beautiful table. Then I think of all the possible events that could have stopped this moment from ever happening, and it all feels, I don\\'t know...\" \"What?\" \"So fragile.\" Now he becomes thoughtful for a moment. He says finally, \"It\\'s terrifying when you consider that every thought we have, every choice we could possibly make, branches into a new world.\" \\n (view spoiler)[This book can\\'t be discussed without spoilers. It is a book about choice and regret. Ever regret not chasing the girl of your dreams so you can focus on your career? Well Jason2 made that choice and then did regret it. Clearly the author is trying to tell us to optimize for happiness - to be that second rate physics teacher at a community college if it means you can have a happy life. I\\'m being snarky because while there is certainly something to that, you also have to have meaning in your life that comes from within. I thought the book was a little shallow on this dimension. In fact, all the characters were fairly shallow. Daniela was the perfect wife. Ryan the perfect antithesis of Jason. Amanda the perfect loyal traveling companion, etc. This, plus the fact that the book was weak on the science are what led me to take a few stars off - but I\\'d still read it again if I could go back in time - was a very fun and engaging read. \\n If you want to really minimize regret, you have to live your life to avoid it in the first place. Regret can\\'t be hacked, which is kind of the point of the book. My favorite book about regret is Remains of the Day. I do really like the visualization of the decision tree though - that is a powerful concept. \\n \"Every moment, every breath, contains a choice. But life is imperfect. We make the wrong choices. So we end up living in a state of perpetual regret, and is there anything worse? I built something that could actually eradicate regret. Let you find worlds where you made the right choice.\" Daniela says, \"Life doesn\\'t work that way. You live with your choices and learn. You don\\'t cheat the system.\" \\n (hide spoiler)]'],\n       ...,\n       [b'** spoiler alert ** \\n 3.5 stars. \\n This book is sweet inside and out! What\\'s sweeter than the ice cream on this cover??? and Patrick...he\\'s such a sweet guy! \\n Elyse hates Valentine\\'s Day, because a year ago, on that very same day she was betrayed by the two people she loved. She caught her best friend and boyfriend cheating on her! Now, she is working for a gift/card store with her new friend Dina and seriously annoyed by the singing Cupid that every customer seemed to love. After her last relationship, she vowed not to be involved with anyone for a while and decided to focus in school and work. Dina, on the other hand, can\\'t seem to get over her ex-boyfriend. Elyse wants to help her friend to forget her ex, so when a cute guy came in the store, she believe that she just found the perfect distraction for Dina. The cute guy---Patrick have a mind and feelings of his own, and he\\'s on a mission to show Elyse that he\\'s nothing like her ex-boyfriend! \\n Elyse was a little annoying, however, I understand where she is coming from. I guess if you were betrayed and got hurt, your initial reaction is to protect yourself. You\\'ll be scared to take risks and you tend to push people away. It\\'s also harder to trust and open up to people. That\\'s what Elyse was doing in this story. When she and her mom moved because of financial reasons (this is right after \"the break-up\"), she never made friends in her new school, aside from Dina. That\\'s why when she noticed Patrick\\'s advances, she immediately shut him out. I just love Patrick. He\\'s too good to be true! He\\'s sweet and thoughtful, and I totally hate Elyse now for having him. LOL! \\n I liked the story and the characters,the book made me smile despite my sickness. This book is perfect for people who wants to read light-hearted teen romance novels. \\n Now I\\'m off to find MY Patrick! =D'],\n       [b'** spoiler alert ** \\n Another fun read from Ms Evanovich! \\n Diesel and Lizzy\\'s new assignment is to find the next SALIGIA stone which is Luxuria also known as the \"lust stone\". Of course Wulf and his crazy assistant Hatchett is also looking for the same stone, but there\\'s a new player in the mix---Anarchy. \\n There was a lot of action that occured in this book than in Wicked Appetite. As always, Glo and the Carl\\'s antics made me laugh. I\\'m glad that this two characters were used in finding the clues that lead Diesel and Lizzy closer to the Luxuria stone. I was surprised that Carl was the \"innocent\" one when he loves flipping people off. LOL! I really enjoyed their scavenger hunt. The Hatchett-Glo love team was hilarious. I just wish that it was Diesel or Wulf who got affected by the Luxuria and not Hatchet. Morty and his spoon bending ability was funny too! I think my favorite addition in this book is the exploding cars! LOL. I still want to know more about Wulf, It seems like he\\'s really not that bad. I hope we get to know him better in the next book! \\n I had fun reading this book. Even though I was feeling under the weather, this book kept me well entertained and awake all through out my work shift. Recommended for people who are looking for something light and funny book to read.'],\n       [b\"** spoiler alert ** \\n 3.5 stars \\n I liked it! The story is original and it's well written. I find Jane a very likable character, she's smart and confident. However, I just don't get what she liked so much about Elton, he's so not worth it! The other characters are not so memorable. Her best friend for example, her character is flat. I didn't learn much about her to be sad or frustrated when she supposedly lost her soul to Lanalee. I liked Owen though, even if he's more than a hundred years old, he's still charming and swoon worthy. Though I liked the idea of him and Jane fighting demons together, I just wished they had more time getting to know each other before they decided to be boyfriend-girlfriend. \\n Now my question is: why there isn't a sequel for Devilish? \\n This was a quick and entertaining read. I recommend it for YA paranormal fans who are looking for something light and unpredictable read.\"]],\n      dtype=object)>"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = ['review_text']\n",
    "features = df[features_names]\n",
    "tf.convert_to_tensor(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_spoilers = tf.strings.regex_replace(lowercase, '\\*\\* spoiler alert \\*\\*', ' ')\n",
    "    stripped_ponctuation = tf.strings.regex_replace(stripped_spoilers, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
    "    data = []\n",
    "    for i in stopwords:\n",
    "        data = tf.strings.regex_replace(stripped_ponctuation, f' {i} ', \" \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "max_features = 5000  # Maximum vocab size.\n",
    "sequence_length = 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "vectorized_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "vectorized_layer.adapt(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "model_nb = 1\n",
    "\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 8000\n",
    "dropout_rate = 0.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_10 (TextVec  (None, 100)         0           ['input_12[0][0]']               \n",
      " torization)                                                                                      \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 100, 50)      250050      ['text_vectorization_10[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 98, 64)       9664        ['embedding_8[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 98, 64)      256         ['conv1d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 98, 64)       0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 94, 128)      41088       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 94, 128)     512         ['conv1d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 94, 128)      0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 90, 256)      164096      ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 90, 256)      141056      ['embedding_8[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 90, 256)     1024        ['conv1d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 90, 256)     1024        ['conv1d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 90, 256)     0           ['batch_normalization_34[0][0]', \n",
      " mbda)                                                            'batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " global_max_pooling1d_8 (Global  (None, 256)         0           ['tf.__operators__.add_8[0][0]'] \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 32)           8224        ['global_max_pooling1d_8[0][0]'] \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 5)            165         ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 617,159\n",
      "Trainable params: 615,751\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "vectorized_text = vectorized_layer(input_text)\n",
    "\n",
    "embedding_layer = Embedding(max_features + 1, embedding_dim, input_length=sequence_length)(vectorized_text)\n",
    "\n",
    "x_shortcut = embedding_layer\n",
    "\n",
    "#### Main path ####\n",
    "# First\n",
    "x = Conv1D(64, 3, activation='relu', padding = 'valid')(embedding_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Second\n",
    "x = Conv1D(128, 5, activation='relu', padding = 'valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Third\n",
    "x = Conv1D(256, 5, activation='relu', padding = 'valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#### Shortcut path ####\n",
    "x_shortcut = Conv1D(256, 11, activation='relu', padding = 'valid')(x_shortcut)\n",
    "x_shortcut = BatchNormalization()(x_shortcut)\n",
    "\n",
    "# x and x_shortcut addition\n",
    "x = x + x_shortcut\n",
    "\n",
    "global_max_pooling = GlobalMaxPooling1D()(x)\n",
    "\n",
    "relu = Dense(32, activation='relu')(global_max_pooling)\n",
    "\n",
    "output = Dense(6, activation='softmax')(relu)\n",
    "\n",
    "resnet_model = Model(input_text, output)\n",
    "\n",
    "resnet_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "resnet_model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate=learning_rate),\n",
    "                  metrics=sparse_categorical_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "exp_name = f'resnet_model_{model_nb}_lr_{learning_rate}_bs_{batch_size}_dr_{dropout_rate}'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "82/82 [==============================] - 67s 799ms/step - loss: 1.6147 - sparse_categorical_accuracy: 0.3867 - val_loss: 1.8927 - val_sparse_categorical_accuracy: 0.3615\n",
      "Epoch 2/50\n",
      "82/82 [==============================] - 65s 795ms/step - loss: 1.1413 - sparse_categorical_accuracy: 0.4980 - val_loss: 3.2634 - val_sparse_categorical_accuracy: 0.3615\n",
      "Epoch 3/50\n",
      "82/82 [==============================] - 65s 795ms/step - loss: 1.0373 - sparse_categorical_accuracy: 0.5429 - val_loss: 3.8985 - val_sparse_categorical_accuracy: 0.3615\n",
      "Epoch 4/50\n",
      "82/82 [==============================] - 65s 795ms/step - loss: 0.9850 - sparse_categorical_accuracy: 0.5681 - val_loss: 3.8801 - val_sparse_categorical_accuracy: 0.3615\n",
      "Epoch 5/50\n",
      "82/82 [==============================] - 65s 795ms/step - loss: 0.9476 - sparse_categorical_accuracy: 0.5877 - val_loss: 2.9152 - val_sparse_categorical_accuracy: 0.3616\n",
      "Epoch 6/50\n",
      "82/82 [==============================] - 65s 798ms/step - loss: 0.9156 - sparse_categorical_accuracy: 0.6048 - val_loss: 1.5078 - val_sparse_categorical_accuracy: 0.4155\n",
      "Epoch 7/50\n",
      "82/82 [==============================] - 65s 796ms/step - loss: 0.8868 - sparse_categorical_accuracy: 0.6202 - val_loss: 1.2604 - val_sparse_categorical_accuracy: 0.4005\n",
      "Epoch 8/50\n",
      "82/82 [==============================] - 65s 796ms/step - loss: 0.8583 - sparse_categorical_accuracy: 0.6355 - val_loss: 1.1240 - val_sparse_categorical_accuracy: 0.4748\n",
      "Epoch 9/50\n",
      "82/82 [==============================] - 65s 796ms/step - loss: 0.8297 - sparse_categorical_accuracy: 0.6500 - val_loss: 1.0770 - val_sparse_categorical_accuracy: 0.5112\n",
      "Epoch 10/50\n",
      "82/82 [==============================] - 65s 797ms/step - loss: 0.8036 - sparse_categorical_accuracy: 0.6637 - val_loss: 1.0914 - val_sparse_categorical_accuracy: 0.5182\n",
      "Epoch 11/50\n",
      "82/82 [==============================] - 66s 808ms/step - loss: 0.7758 - sparse_categorical_accuracy: 0.6783 - val_loss: 1.1393 - val_sparse_categorical_accuracy: 0.5103\n",
      "Epoch 12/50\n",
      "82/82 [==============================] - 66s 810ms/step - loss: 0.7495 - sparse_categorical_accuracy: 0.6907 - val_loss: 1.1600 - val_sparse_categorical_accuracy: 0.5109\n",
      "Epoch 13/50\n",
      "82/82 [==============================] - 66s 808ms/step - loss: 0.7204 - sparse_categorical_accuracy: 0.7050 - val_loss: 1.2145 - val_sparse_categorical_accuracy: 0.5053\n",
      "Epoch 14/50\n",
      "82/82 [==============================] - 67s 819ms/step - loss: 0.6924 - sparse_categorical_accuracy: 0.7184 - val_loss: 1.2594 - val_sparse_categorical_accuracy: 0.4954\n",
      "Epoch 15/50\n",
      "82/82 [==============================] - 68s 835ms/step - loss: 0.6670 - sparse_categorical_accuracy: 0.7300 - val_loss: 1.2870 - val_sparse_categorical_accuracy: 0.4988\n",
      "Epoch 16/50\n",
      "82/82 [==============================] - 68s 834ms/step - loss: 0.6415 - sparse_categorical_accuracy: 0.7416 - val_loss: 1.3233 - val_sparse_categorical_accuracy: 0.4945\n",
      "Epoch 17/50\n",
      "82/82 [==============================] - 68s 835ms/step - loss: 0.6141 - sparse_categorical_accuracy: 0.7548 - val_loss: 1.3693 - val_sparse_categorical_accuracy: 0.4914\n",
      "Epoch 18/50\n",
      "82/82 [==============================] - 68s 837ms/step - loss: 0.5908 - sparse_categorical_accuracy: 0.7651 - val_loss: 1.4259 - val_sparse_categorical_accuracy: 0.4902\n",
      "Epoch 19/50\n",
      "82/82 [==============================] - 68s 837ms/step - loss: 0.5712 - sparse_categorical_accuracy: 0.7731 - val_loss: 1.4617 - val_sparse_categorical_accuracy: 0.4942\n",
      "Epoch 20/50\n",
      "82/82 [==============================] - 68s 834ms/step - loss: 0.5426 - sparse_categorical_accuracy: 0.7872 - val_loss: 1.5055 - val_sparse_categorical_accuracy: 0.4859\n",
      "Epoch 21/50\n",
      "82/82 [==============================] - 68s 834ms/step - loss: 0.5181 - sparse_categorical_accuracy: 0.7978 - val_loss: 1.5462 - val_sparse_categorical_accuracy: 0.4864\n",
      "Epoch 22/50\n",
      "82/82 [==============================] - 68s 835ms/step - loss: 0.4952 - sparse_categorical_accuracy: 0.8084 - val_loss: 1.6221 - val_sparse_categorical_accuracy: 0.4804\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - 68s 836ms/step - loss: 0.4728 - sparse_categorical_accuracy: 0.8179 - val_loss: 1.6680 - val_sparse_categorical_accuracy: 0.4795\n",
      "Epoch 24/50\n",
      "82/82 [==============================] - 68s 836ms/step - loss: 0.4571 - sparse_categorical_accuracy: 0.8233 - val_loss: 1.7317 - val_sparse_categorical_accuracy: 0.4757\n",
      "Epoch 25/50\n",
      "82/82 [==============================] - 68s 834ms/step - loss: 0.4416 - sparse_categorical_accuracy: 0.8296 - val_loss: 1.7893 - val_sparse_categorical_accuracy: 0.4729\n",
      "Epoch 26/50\n",
      "82/82 [==============================] - 68s 836ms/step - loss: 0.4256 - sparse_categorical_accuracy: 0.8363 - val_loss: 1.8256 - val_sparse_categorical_accuracy: 0.4801\n",
      "Epoch 27/50\n",
      "82/82 [==============================] - 68s 834ms/step - loss: 0.4028 - sparse_categorical_accuracy: 0.8467 - val_loss: 1.9214 - val_sparse_categorical_accuracy: 0.4675\n",
      "Epoch 28/50\n",
      "82/82 [==============================] - 68s 835ms/step - loss: 0.3924 - sparse_categorical_accuracy: 0.8501 - val_loss: 1.9310 - val_sparse_categorical_accuracy: 0.4776\n",
      "Epoch 29/50\n",
      "82/82 [==============================] - 68s 833ms/step - loss: 0.3771 - sparse_categorical_accuracy: 0.8560 - val_loss: 1.9870 - val_sparse_categorical_accuracy: 0.4784\n",
      "Epoch 30/50\n",
      "82/82 [==============================] - 68s 834ms/step - loss: 0.3594 - sparse_categorical_accuracy: 0.8640 - val_loss: 2.0571 - val_sparse_categorical_accuracy: 0.4788\n",
      "Epoch 31/50\n",
      "82/82 [==============================] - 68s 835ms/step - loss: 0.3434 - sparse_categorical_accuracy: 0.8712 - val_loss: 2.1072 - val_sparse_categorical_accuracy: 0.4774\n",
      "Epoch 32/50\n",
      "82/82 [==============================] - 68s 833ms/step - loss: 0.3317 - sparse_categorical_accuracy: 0.8753 - val_loss: 2.1476 - val_sparse_categorical_accuracy: 0.4728\n",
      "Epoch 33/50\n",
      "82/82 [==============================] - 68s 829ms/step - loss: 0.3156 - sparse_categorical_accuracy: 0.8825 - val_loss: 2.2262 - val_sparse_categorical_accuracy: 0.4680\n",
      "Epoch 34/50\n",
      "20/82 [======>.......................] - ETA: 42s - loss: 0.2619 - sparse_categorical_accuracy: 0.9090"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[157], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mresnet_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m              \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m              \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m              \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensorBoard\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogs/resnets/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mexp_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_2.10_py_3.10\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "resnet_model.fit(features,\n",
    "              targets,\n",
    "              validation_split=0.25,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[tf.keras.callbacks.TensorBoard(\"logs/resnets/\" + exp_name)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(result_path, sep=\",\")\n",
    "\n",
    "df_test_modified = df_test.drop(columns=[\n",
    "    'user_id',\n",
    "    'book_id',\n",
    "    'review_id',\n",
    "    'date_added',\n",
    "    'date_updated',\n",
    "    'read_at',\n",
    "    'started_at',\n",
    "    'n_votes',\n",
    "    'n_comments'\n",
    "], inplace=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_test_data = resnet_model.predict(df_test_modified)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_test_data_to_result_csv(df_test, predicted_test_data, exp_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
